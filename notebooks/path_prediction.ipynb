{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import LineString\n",
    "import missingno as msn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import geohash2\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Dense, Dropout, Activation, Masking\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Flatten, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all Atlantic hurricane data\n",
    "hurricanes = pd.read_csv('../data/atlantic_hurricanes.csv')\n",
    "\n",
    "# Create a date column and only use hurricanes since 1990\n",
    "hurricanes['Date'] = pd.to_datetime(hurricanes['Date'], errors='coerce')\n",
    "hurricanes.dropna(inplace=True)\n",
    "hurricanes['year'] = hurricanes['Date'].dt.year.astype(int)\n",
    "hurricanes = hurricanes[hurricanes['year'] >= 1990]\n",
    "hurricanes = hurricanes[hurricanes['Name'] != 'Unnamed']\n",
    "\n",
    "# ETL to make lat/long correct, and convert them to shapely points\n",
    "hurricanes['slug'] = hurricanes['Name'] + '-' + hurricanes['year'].astype(str)\n",
    "hurricanes['Long'] = 0 - hurricanes['Long']\n",
    "hurricanes['coordinates'] = hurricanes[['Long', 'Lat']].values.tolist()\n",
    "hurricanes['coordinates'] = hurricanes['coordinates'].apply(Point)\n",
    "\n",
    "# Extract the movement speed of the hurricane as a feature\n",
    "hurricanes['movement_speed'] = hurricanes['Movement'].str.extract(r'(\\d+)\\s?[mph|MPH]')\n",
    "hurricanes.fillna(value=0, inplace=True)\n",
    "\n",
    "# Create geohashes from the lat/long for use in modeling\n",
    "geohashes = []\n",
    "\n",
    "for index,row in hurricanes.iterrows():\n",
    "    latitude = row['coordinates'].x\n",
    "    longitude = row['coordinates'].y\n",
    "    geohash = geohash2.encode(\n",
    "        latitude=latitude, \n",
    "        longitude=longitude, \n",
    "        precision=5\n",
    "    )\n",
    "    geohashes.append(geohash)\n",
    "    \n",
    "hurricanes['geohash'] = geohashes\n",
    "n_classes = len(set(hurricanes['geohash']))\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Encode the geohash labels as integers\n",
    "hurricanes['encoded_label'] = label_encoder.fit_transform(hurricanes['geohash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Lat', 'Long', 'Wind', 'Pres', 'movement_speed']\n",
    "label = 'encoded_label'\n",
    "test_slug = 'Katrina-2005'\n",
    "\n",
    "# Scale the input data between 0-1\n",
    "for feature in features:\n",
    "    values = hurricanes[feature].values\n",
    "    hurricanes[feature] = min_max_scaler.fit_transform(\n",
    "        values.reshape(-1, 1)\n",
    "    )\n",
    "\n",
    "pre_train_x = []\n",
    "pre_train_y = []\n",
    "\n",
    "pre_test_x = []\n",
    "pre_test_y = []\n",
    "\n",
    "# Create tensors of the feature and labels\n",
    "for name, group in hurricanes.groupby('slug'):\n",
    "    temp_df = hurricanes[hurricanes['slug'] == name]\n",
    "    if name == test_slug:\n",
    "        pre_test_x.append(temp_df[features].to_numpy())\n",
    "        pre_test_y.append(temp_df[label].to_numpy())\n",
    "    if len(temp_df) >= 40:\n",
    "        pre_train_x.append(temp_df[features].to_numpy())\n",
    "        pre_train_y.append(temp_df[label].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the hurricanes paths need to be the same length\n",
    "# We can use a kera tool to do this.\n",
    "def pad_sequence(data):\n",
    "    padded = pad_sequences(\n",
    "        sequences=data,\n",
    "        maxlen=50,\n",
    "        dtype='object',\n",
    "        padding='post',\n",
    "        truncating='pre',\n",
    "        value=0.0\n",
    "    )\n",
    "                   \n",
    "    return padded\n",
    "\n",
    "# Build the layer structures of the RNN\n",
    "def build_structure():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units=300,\n",
    "        input_shape=(50, 5),\n",
    "        activation='relu', \n",
    "        recurrent_activation='hard_sigmoid', \n",
    "        return_sequences=True,\n",
    "        dropout=0.1,\n",
    "        recurrent_dropout=0.1\n",
    "    ))\n",
    "        \n",
    "    # Output layer\n",
    "    model.add(Dense(\n",
    "        units=n_classes, \n",
    "        activation='softmax'\n",
    "    ))\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        optimizer='Adagrad',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_train_x = pad_sequence(data=pre_train_x)\n",
    "post_train_y = pad_sequence(data=pre_train_y)\n",
    "post_train_y = post_train_y.reshape(\n",
    "    np.shape(pre_train_x)[0], 50, 1\n",
    ")\n",
    "post_test_x = pad_sequence(data=pre_test_x)\n",
    "post_test_y = pad_sequence(data=pre_test_y)\n",
    "post_test_y = post_test_y.reshape(1, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74 samples, validate on 33 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 10.0848 - categorical_accuracy: 0.5600 - val_loss: 14.4575 - val_categorical_accuracy: 0.7703\n",
      "Epoch 2/10\n",
      " - 2s - loss: 12.3965 - categorical_accuracy: 0.6732 - val_loss: 9.3666 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      " - 2s - loss: 9.2880 - categorical_accuracy: 0.0000e+00 - val_loss: 12.0730 - val_categorical_accuracy: 0.4236\n",
      "Epoch 4/10\n",
      " - 2s - loss: 10.0741 - categorical_accuracy: 0.1843 - val_loss: 9.3850 - val_categorical_accuracy: 0.0036\n",
      "Epoch 5/10\n",
      " - 2s - loss: 9.2306 - categorical_accuracy: 0.0141 - val_loss: 9.4111 - val_categorical_accuracy: 0.0970\n",
      "Epoch 6/10\n",
      " - 2s - loss: 9.9650 - categorical_accuracy: 0.3251 - val_loss: 9.3989 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      " - 2s - loss: 9.1858 - categorical_accuracy: 0.0024 - val_loss: 9.4139 - val_categorical_accuracy: 0.0048\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.2588 - categorical_accuracy: 0.0173 - val_loss: 9.4030 - val_categorical_accuracy: 0.0170\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.1764 - categorical_accuracy: 0.1176 - val_loss: 9.4153 - val_categorical_accuracy: 0.0388\n",
      "Epoch 10/10\n",
      " - 2s - loss: 9.1461 - categorical_accuracy: 0.2786 - val_loss: 9.4013 - val_categorical_accuracy: 0.0994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f26677a5f98>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_structure()\n",
    "model.fit(\n",
    "    x=post_train_x,\n",
    "    y=post_train_y,\n",
    "    epochs=10,\n",
    "    verbose=2,\n",
    "    validation_split=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11149, 10829, 10829, 10829, 10829, 10829, 10829, 10829, 10829,\n",
       "        10829, 10829, 10829, 10829, 10829, 10829, 10829, 10829, 10829,\n",
       "        10829, 10829, 10829, 10829, 10829, 10829, 10829, 10829, 10829,\n",
       "        10829, 10829, 10829, 10829, 10829, 10829, 10829, 10829, 10829,\n",
       "        10829, 10829, 10829, 10829, 10829, 10829, 10829, 10829, 10829,\n",
       "        10829, 10829, 10829, 10829, 10829]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(post_test_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
